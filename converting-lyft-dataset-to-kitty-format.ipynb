{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this kernel we convert LEVEL5 Lyft data (NuScenes format) to KITTI format, which is usually used in public repositories. After this you can search for repos, that solve KITTI 3d-detection task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "!pip install -qqq -U git+https://github.com/stalkermustang/nuscenes-devkit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir with all input data from Kaggle\n",
    "INP_DIR = Path('/Users/kanhua/Downloads/3d-object-detection-for-autonomous-vehicles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir with index json tables (scenes, categories, logs, etc...)\n",
    "TABLES_DIR = INP_DIR.joinpath('train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: images/train_images: File exists\n",
      "ln: maps/train_maps: File exists\n",
      "ln: lidar/train_lidar: File exists\n"
     ]
    }
   ],
   "source": [
    "# Adjust the dataroot parameter below to point to your local dataset path.\n",
    "# The correct dataset path contains at least the following four folders (or similar): images, lidar, maps\n",
    "!ln -s {INP_DIR}/train_images images\n",
    "!ln -s {INP_DIR}/train_maps maps\n",
    "!ln -s {INP_DIR}/train_lidar lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path().absolute() \n",
    "# Empty init equals '.'.\n",
    "# We use this because we link train dirs to current dir (cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir to write KITTY-style dataset\n",
    "STORE_DIR = DATA_DIR.joinpath('kitti_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Showing help with the command 'export_kitti.py nuscenes_gt_to_kitti -- --help'.\r\n",
      "\r\n",
      "\u001b[1mNAME\u001b[0m\r\n",
      "    export_kitti.py nuscenes_gt_to_kitti - Converts nuScenes GT fromatted annotations to KITTI format.\r\n",
      "\r\n",
      "\u001b[1mSYNOPSIS\u001b[0m\r\n",
      "    export_kitti.py nuscenes_gt_to_kitti \u001b[4mLYFT_DATAROOT\u001b[0m \u001b[4mTABLE_FOLDER\u001b[0m <flags>\r\n",
      "\r\n",
      "\u001b[1mDESCRIPTION\u001b[0m\r\n",
      "    Converts nuScenes GT fromatted annotations to KITTI format.\r\n",
      "\r\n",
      "\u001b[1mPOSITIONAL ARGUMENTS\u001b[0m\r\n",
      "    \u001b[1m\u001b[4mLYFT_DATAROOT\u001b[0m\u001b[0m\r\n",
      "        Where lyft dataset stored (root dir).\r\n",
      "    \u001b[1m\u001b[4mTABLE_FOLDER\u001b[0m\u001b[0m\r\n",
      "        folder with tables (json files).\r\n",
      "\r\n",
      "\u001b[1mFLAGS\u001b[0m\r\n",
      "    --lidar_name=\u001b[4mLIDAR_NAME\u001b[0m\r\n",
      "        Name of the lidar sensor. Only one lidar allowed at this moment.\r\n",
      "    --get_all_detections=\u001b[4mGET_ALL_DETECTIONS\u001b[0m\r\n",
      "        If True, will write all bboxes in PointCloud and use only FrontCamera.\r\n",
      "    --parallel_n_jobs=\u001b[4mPARALLEL_N_JOBS\u001b[0m\r\n",
      "        Number of threads to parralel processing.\r\n",
      "    --samples_count=\u001b[4mSAMPLES_COUNT\u001b[0m\r\n",
      "        Number of samples to convert.\r\n",
      "\r\n",
      "\u001b[1mNOTES\u001b[0m\r\n",
      "    You can also use flags syntax for POSITIONAL ARGUMENTS\r\n"
     ]
    }
   ],
   "source": [
    "!python -m lyft_dataset_sdk.utils.export_kitti nuscenes_gt_to_kitti -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "18421 instance,\n",
      "10 sensor,\n",
      "148 calibrated_sensor,\n",
      "177789 ego_pose,\n",
      "180 log,\n",
      "180 scene,\n",
      "22680 sample,\n",
      "189504 sample_data,\n",
      "638179 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 10.8 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 3.9 seconds.\n",
      "======\n",
      "100%|███████████████████████████████████████| 2000/2000 [11:33<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# convertation to KITTY-format\n",
    "!python -m lyft_dataset_sdk.utils.export_kitti nuscenes_gt_to_kitti \\\n",
    "        --lyft_dataroot {DATA_DIR} \\\n",
    "        --table_folder {TABLES_DIR} \\\n",
    "        --samples_count 2000 \\\n",
    "        --parallel_n_jobs 2 \\\n",
    "        --get_all_detections True \\\n",
    "        --store_dir {STORE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0032f6f11828b3925466a7699fbe598938a652560260c87f4742aa164b1d5021.bin\r\n",
      "006385e4f70c8f0c98ea85dbc7b89c054f9ae7f019332f975716cf8d45f20a03.bin\r\n"
     ]
    }
   ],
   "source": [
    "# check created (converted) files. velodyne = LiDAR poinclouds data (in binary)\n",
    "!ls {STORE_DIR}/velodyne | head -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering 3d boxes projected from 3d KITTI format\n",
      "  2%|▉                                      | 46/2000 [03:20<2:52:09,  5.29s/it]Traceback (most recent call last):\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/site-packages/lyft_dataset_sdk/utils/export_kitti.py\", line 316, in <module>\n",
      "    fire.Fire(KittiConverter)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/site-packages/fire/core.py\", line 138, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/site-packages/fire/core.py\", line 471, in _Fire\n",
      "    target=component.__name__)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/site-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/site-packages/lyft_dataset_sdk/utils/export_kitti.py\", line 295, in render_kitti\n",
      "    token, sensor_modality=sensor, out_path=out_path, render_2d=render_2d)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/site-packages/lyft_dataset_sdk/utils/kitti.py\", line 499, in render_sample_data\n",
      "    plt.savefig(out_path)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/site-packages/matplotlib/pyplot.py\", line 722, in savefig\n",
      "    res = fig.savefig(*args, **kwargs)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/site-packages/matplotlib/figure.py\", line 2180, in savefig\n",
      "    self.canvas.print_figure(fname, **kwargs)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 2082, in print_figure\n",
      "    **kwargs)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 530, in print_png\n",
      "    cbook.open_file_cm(filename_or_obj, \"wb\") as fh:\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/contextlib.py\", line 112, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\", line 447, in open_file_cm\n",
      "    fh, opened = to_filehandle(path_or_file, mode, True, encoding)\n",
      "  File \"/Users/kanhua/miniconda3/envs/convert-kitti/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\", line 432, in to_filehandle\n",
      "    fh = open(fname, flag, encoding=encoding)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/Users/kanhua/Dropbox/Programming/lyft-3d-main/kitti_format/render/063ea8386aa8156c1cd7c91bb1ac433f0b58d471e126588ccaa17ababb6b9b7a_lidar.png'\n",
      "  2%|▉                                      | 46/2000 [03:22<2:23:21,  4.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# render converted data for check. Currently don't support multithreading :(\n",
    "!python -m lyft_dataset_sdk.utils.export_kitti render_kitti \\\n",
    "        --store_dir {STORE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script above write images to 'render' folder\n",
    "# in store_dir (where we have converted dataset)\n",
    "RENDER_DIR = STORE_DIR.joinpath('render')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all rendered files\n",
    "all_renders = list(RENDER_DIR.glob('*'))\n",
    "all_renders.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render radar data (bird view) and camera data with bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-898447be425a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_renders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "Image.open(all_renders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(all_renders[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I'm use rendering only for check success converting. \n",
    "\n",
    "## Can be used to visualize NN predictions for test lyft set (visual metric estimation :D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!rm -rf {STORE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
